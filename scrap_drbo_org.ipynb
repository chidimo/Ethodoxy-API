{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://drbo.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from django.template.defaultfilters import slugify\n",
    "from pywebber import Ripper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "domain = \"http://drbo.org/\"\n",
    "base = \"/home/parousia/catholic\"\n",
    "# base = \"D:\\git\\catholic\"\n",
    "save_path = os.path.join(base, \"drbo_org_scrap\")\n",
    "data_store = os.path.join(base, \"drbo_data\")\n",
    "chapter_store = os.path.join(base, \"drbo_data\", \"chapters\")\n",
    "verse_store = os.path.join(base, \"drbo_data\", \"verses\")\n",
    "challoner_store = os.path.join(base, \"drbo_data\", \"challoner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_filename(file_name):\n",
    "    return \"_\".join([each.lower() for each in re.split(r\"[\\, *, \\/]\", file_name) if each != ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "home = Ripper(domain, save_path=\"D:\\git\\catholic\\drbo_org_scrap\")\n",
    "home_links = list(home.links())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_books_page_links(raw_page_rip):\n",
    "    \"\"\"Get page links for each book\"\"\"\n",
    "    nt = {}\n",
    "    ot = {}\n",
    "    OTIDS = []\n",
    "    soup = raw_page_rip.soup\n",
    "    if not os.path.exists(data_store):\n",
    "        os.mkdir(data_store)\n",
    "    \n",
    "    nt_soup = soup.find(\"td\", class_=\"NT\")\n",
    "    ot1 = soup.find(\"td\", class_=\"OT1\")\n",
    "    ot2 = soup.find(\"td\", class_=\"OT2\")\n",
    "    \n",
    "    for each in nt_soup.find_all(\"a\", href=True):\n",
    "        if 'class=\"b\"' in str(each):\n",
    "            href = each.get(\"href\")\n",
    "            name = each.text\n",
    "\n",
    "            idd = re.search(r'\\d{5}', href).group(0)\n",
    "            nt[name] = [domain + href, idd]\n",
    "            \n",
    "    with open(os.path.join(data_store, \"new_test.json\"), \"w+\") as wh:\n",
    "        json.dump(nt, wh)\n",
    "\n",
    "    for each in ot1.find_all(\"a\", href=True):\n",
    "        if 'class=\"b\"' in str(each):\n",
    "            \n",
    "            href = each.get(\"href\")\n",
    "            name = each.text\n",
    "            idd = re.search(r'\\d{5}', href).group(0)\n",
    "                        \n",
    "            if idd in OTIDS:\n",
    "                ot[domain + href][0] = name + \" or \" + ot[domain + href][0]\n",
    "            else:\n",
    "                ot[domain + href] = [name, idd]\n",
    "                OTIDS.append(idd)\n",
    "                \n",
    "    for each in ot2.find_all(\"a\", href=True):\n",
    "        if 'class=\"b\"' in str(each):\n",
    "            \n",
    "            href = each.get(\"href\")\n",
    "            name = each.text\n",
    "            idd = re.search(r'\\d{5}', href).group(0)\n",
    "                        \n",
    "            if idd in OTIDS:\n",
    "                ot[domain + href][0] = name + \" or \" + ot[domain + href][0]\n",
    "            else:\n",
    "                ot[domain + href] = [name, idd]\n",
    "                OTIDS.append(idd)\n",
    "                \n",
    "    rev_old = {value[0] : [key, value[1]] for key, value in ot.items()}\n",
    "    with open(os.path.join(data_store, \"old_test.json\"), \"w+\") as wh:\n",
    "        json.dump(rev_old, wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def export_chapter_links_to_json(book_name, book_link, idd):\n",
    "    \"\"\"Get all chapters and write to json\"\"\"\n",
    "    chapters = {\"01\" : book_link}\n",
    "    home = Ripper(book_link, save_path=save_path)\n",
    "    \n",
    "    if not os.path.exists(chapter_store):\n",
    "        os.mkdir(chapter_store)\n",
    "    \n",
    "    for each in home.raw_links:\n",
    "        str_each = str(each)\n",
    "        excludes = [\"next\" in str_each, \"previous\" in str_each, \"chapter\" in str_each, \"statcounter\" in str_each,\n",
    "                    \"/x/\" in str_each, \"DRBO.ORG\" in str_each, \"theologica\" in str_each]\n",
    "        \n",
    "        if not any(excludes):\n",
    "            chapters[each.text] = domain + \"chapter/\" + each.get(\"href\")\n",
    "\n",
    "    book_chaps = {book_name : chapters}\n",
    "    \n",
    "    book_save_name = \"{}.json\".format(os.path.join(chapter_store, normalize_filename(book_name)))\n",
    "\n",
    "    with open(book_save_name, \"w+\") as wh:\n",
    "        json.dump(book_chaps, wh)\n",
    "    return book_chaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join_chapter_text(chapter_content_list):\n",
    "    \"\"\"Join paragraphs of a chapter\"\"\"\n",
    "    cleaned_chapter_content_list = [each for each in chapter_content_list if each != \"\\n\"]\n",
    "    chap_text = {0 : \"\"}\n",
    "    tracker = 0\n",
    "\n",
    "    for each in chapter_content_list:\n",
    "        each = str(each)\n",
    "        if re.search(r'\\/x\\/d\\?b=drb', each):\n",
    "            tracker += 1\n",
    "            verse = re.search(r'\\[(\\d+)\\]', each).group(1)\n",
    "            chap_text[tracker] = \"\"\n",
    "        else:\n",
    "            chap_text[tracker] = chap_text[tracker] + each.strip()\n",
    "    \n",
    "    chap_text.pop(0, None)\n",
    "    return chap_text\n",
    "\n",
    "def get_chapter_text(location):\n",
    "    \"\"\"Get text content for a single chapter\"\"\"\n",
    "    chapter_output_dictionary = {}\n",
    "    chapter_contents_list = []\n",
    "\n",
    "    soup = Ripper(location, parser=\"html5lib\", save_path=save_path).soup\n",
    "    text = soup.find(\"table\", class_=\"texttable\")\n",
    "\n",
    "    for each in text.find_all(\"p\"):\n",
    "        attributes = each.attrs\n",
    "        if attributes:\n",
    "            if \"desc\" in attributes[\"class\"]:\n",
    "                pass\n",
    "            elif \"note\" in attributes[\"class\"]:\n",
    "                pass\n",
    "        else:\n",
    "            new_cont = each.contents\n",
    "            chapter_contents_list.extend(new_cont)\n",
    "    return join_chapter_text(chapter_contents_list)\n",
    "\n",
    "def get_all_text_for_book(book_file_name):\n",
    "    \"\"\"Get text for a single book, all chapters\"\"\"\n",
    "    if not os.path.exists(verse_store):\n",
    "        os.mkdir(verse_store)\n",
    "    with open(book_file_name, \"r+\") as rh:\n",
    "        book = json.load(rh)\n",
    "    chapter_text = {}\n",
    "\n",
    "    for name, chapters_dictionary in book.items():\n",
    "        for chap, location in chapters_dictionary.items():\n",
    "            outfile = \"{}/{}_{}.json\".format(verse_store, normalize_filename(name), chap)\n",
    "            \n",
    "            if os.path.exists(outfile):\n",
    "                continue\n",
    "            else:\n",
    "                chapter_text[name + \"__\" + chap] = get_chapter_text(location)\n",
    "                with open(outfile, \"w+\") as wh:\n",
    "                    json.dump(chapter_text, wh)\n",
    "                chapter_text = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dictionary_from_each_commentary_text(commentary_parts_list):\n",
    "    \"\"\"Make a dictionary from each commentary text.\n",
    "    Input is a list consisting of a 3 items.\n",
    "    [verse, underlined text from bible, commentary text]\n",
    "    \"\"\"\n",
    "    verse_string = str(commentary_parts_list[0])\n",
    "    header_string = str(commentary_parts_list[1])\n",
    "    \n",
    "    verse = re.search(r\"\\n\\[(\\d+)\\]\", verse_string).group(1)\n",
    "    header = re.search(r'<u>\"(.+)\"</u>', header_string).group(1)\n",
    "\n",
    "    commentary_text = commentary_parts_list[2].replace(\": \", \"\")\n",
    "    key = verse + \"__\" + header\n",
    "    \n",
    "    return key, commentary_text\n",
    "\n",
    "def get_commentary_for_chapter(location):\n",
    "    \"\"\"Get commentary text for single chapter\"\"\"\n",
    "    chapter_commentary_dictionary = {}\n",
    "\n",
    "    soup = Ripper(location, parser=\"html5lib\", save_path=save_path).soup\n",
    "    text = soup.find(\"table\", class_=\"texttable\")\n",
    "\n",
    "    for each in text.find_all(\"p\"):\n",
    "        attributes = each.attrs\n",
    "        if attributes:\n",
    "            if \"desc\" in attributes[\"class\"]:\n",
    "                pass\n",
    "            elif \"note\" in attributes[\"class\"]:\n",
    "                new_content = each.contents\n",
    "                verse_header, text = make_dictionary_from_each_commentary_text(new_content)\n",
    "                chapter_commentary_dictionary[verse_header] = text\n",
    "        else:\n",
    "            continue\n",
    "    return chapter_commentary_dictionary\n",
    "\n",
    "def get_commentary_for_all_book_chapters(book_file_name):\n",
    "    with open(book_file_name, \"r+\") as rh:\n",
    "        book = json.load(rh)\n",
    "    chapter_text = {}\n",
    "\n",
    "    for name, chapters_dictionary in book.items():\n",
    "        \n",
    "        for chap, location in chapters_dictionary.items():\n",
    "            norm = normalize_filename(\"{}_{}\".format(name, chap))\n",
    "            \n",
    "            outfile = \"{}/{}.json\".format(challoner_store, norm)\n",
    "            \n",
    "            if os.path.exists(outfile):\n",
    "                continue\n",
    "            else:\n",
    "                chapter_text[name + \"__\" + chap] = get_commentary_for_chapter(location)\n",
    "                with open(outfile, \"w+\") as wh:\n",
    "                    json.dump(chapter_text, wh)\n",
    "                chapter_text = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_all_books_page_links(Ripper(url=domain, save_path=save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each in [os.path.join(data_store, \"new_test.json\"), os.path.join(data_store, \"old_test.json\")]:\n",
    "    with open(each, \"r+\") as rh:\n",
    "        books = json.load(rh)\n",
    "    for name, link in books.items():\n",
    "        link = link[0]\n",
    "        idd = link[1]\n",
    "        export_chapter_links_to_json(name, link, idd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_books = glob.glob(\"{}/*.json\".format(chapter_store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for each in all_books:\n",
    "    get_all_text_for_book(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b6a0610b9e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_books\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mget_commentary_for_all_book_chapters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-8773ab9db834>\u001b[0m in \u001b[0;36mget_commentary_for_all_book_chapters\u001b[0;34m(book_file_name)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mchapter_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"__\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchap\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_commentary_for_chapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchapter_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8773ab9db834>\u001b[0m in \u001b[0;36mget_commentary_for_chapter\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"note\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"class\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mnew_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mverse_header\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dictionary_from_each_commentary_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mchapter_commentary_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mverse_header\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8773ab9db834>\u001b[0m in \u001b[0;36mmake_dictionary_from_each_commentary_text\u001b[0;34m(commentary_parts_list)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mheader_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommentary_parts_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\n\\[(\\d+)\\]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverse_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'<u>\"(.+)\"</u>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "for each in all_books:\n",
    "    get_commentary_for_all_book_chapters(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (__ca)",
   "language": "",
   "name": "__ca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
